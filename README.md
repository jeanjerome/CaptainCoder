# CaptainCoder

![CaptainCoder Logo](icon.png)

## Description

CaptainCoder is a privacy-focused, locally-run code completion extension for VSCode that supports any Ollama model and is open-source.

## Features

- AI-powered code completion using customizable models.
- Locally-run to ensure privacy and control over your code.
- Open-source and extensible.
- Configurable prompt and response settings to tailor the autocompletion behavior.

## Installation

1. Install Ollama Locally:
    - Visit [https://ollama.com/](https://ollama.com/) for detailed installation instructions.
2. Download and Configure a Model:
    - Download the desired model and update the `captaincoder.model` configuration accordingly. By default, the configuration is set to `codestral:22b-v0.1-q4_K_M`.

## Usage

1. **Autocomplete Command:**
   - Use the command `CaptainCoder: Autocomplete` to trigger the autocompletion.

2. **Configuration:**
   - The extension can be configured in the VSCode settings under `CaptainCoder`.
   - Available settings include:
     - **endpoint**: The endpoint of the ollama REST API.
     - **model**: The model to use for generating completions.
     - **message header**: Pseudo-system prompt, optimized for code completion.
     - **max tokens predicted**: The maximum number of tokens generated by the model.
     - **prompt window size**: The size of the prompt in characters.
     - **completion keys**: Characters that trigger the autocompletion item provider.
     - **response preview**: Inline completion label as the first line of response.
     - **preview max tokens**: Maximum tokens for the response preview.
     - **preview delay**: Time to wait before starting inline preview generation.
     - **continue inline**: Continue autocompletion after the preview.
     - **temperature**: Temperature of the model.

## Configuration Options

| Setting                           | Type      | Default Value                                             | Description                                                                                           |
| --------------------------------- | --------- | --------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| `captaincoder.endpoint`           | `string`  | `http://localhost:11434/api/generate`                     | The endpoint of the ollama REST API.                                                                  |
| `captaincoder.model`              | `string`  | `codestral:22b-v0.1-q4_K_M`                               | The model to use for generating completions.                                                          |
| `captaincoder.message header`     | `string`  | `The following is a complete {LANG} file named {FILE_NAME}...` | Pseudo-system prompt, optimized for code completion.                                                  |
| `captaincoder.max tokens predicted` | `integer` | 1000                                                    | The maximum number of tokens generated by the model.                                                  |
| `captaincoder.prompt window size` | `integer` | 2000                                                     | The size of the prompt in characters.                                                                 |
| `captaincoder.completion keys`    | `string`  | ` `                                                      | Character that the autocompletion item provider appear on. Multiple characters will be treated as different entries. REQUIRES RELOAD |
| `captaincoder.response preview`   | `boolean` | true                                                     | Inline completion label will be the first line of response.                                           |
| `captaincoder.preview max tokens` | `integer` | 50                                                       | The maximum number of tokens generated by the model for the response preview.                         |
| `captaincoder.preview delay`      | `number`  | 1                                                        | Time to wait in seconds before starting inline preview generation.                                    |
| `captaincoder.continue inline`    | `boolean` | true                                                     | Ollama continues autocompletion after what is previewed inline.                                        |
| `captaincoder.temperature`        | `number`  | 0.5                                                      | Temperature of the model. It is recommended to set it lower than you would for dialogue.              |

## Issues

If you encounter any issues, please report them on the GitHub issues page.

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request.

## License

This project is licensed under the MIT License.
